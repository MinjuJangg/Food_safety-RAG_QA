from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import pandas as pd
import json
import time
import re
import torch

# âœ… ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ
model_path = '/SSL_NAS/concrete/models/models--meta-llama--Meta-Llama-3-8B-Instruct/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e5e23bbe8e749ef0efcf16cad411a7d23bd23298'

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=0)

# íŒŒì´í”„ë¼ì¸ êµ¬ì„± (ì‘ë‹µë§Œ ë°˜í™˜í•˜ë„ë¡ ì„¤ì •)
llama_pipeline = pipeline("text-generation", model=model, tokenizer=tokenizer, return_full_text=False)

# ë¬¸ìì—´ ì•ˆì „ ì²˜ë¦¬ í•¨ìˆ˜
def clean_text(text):
    return str(text).replace('"', "'").strip()

# í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜ (LLaMA3 chat format ê¸°ë°˜)
def make_chat_prompt(title, content):
    system_msg = (
        "ë„ˆëŠ” ì‹í’ˆ ìœ„í•´ ì •ë³´ ë¬¸ì„œ ê¸°ë°˜ QA ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ë„ìš°ë¯¸ì•¼. "
        "ì£¼ì–´ì§„ ë¬¸ì„œì—ì„œ ì§ˆë¬¸(Query)ê³¼ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€(Answer) ìŒì„ ì¶”ì¶œí•´ì•¼ í•´. "
        "íŠ¹íˆ **ê°€ì¥ í•µì‹¬ì ì´ê±°ë‚˜ ì¤‘ìš”í•œ ì§ˆë¬¸ 1ê°œë§Œ** ìƒì„±í•´ì•¼ í•˜ë©°, ì´ëŠ” **ì‚¬ëŒì´ ì´ ë¬¸ì„œë¥¼ ë´¤ì„ ë•Œ ê°€ì¥ ë¨¼ì € ê¶ê¸ˆí•´í•  ì§ˆë¬¸**ì´ì–´ì•¼ í•´. "
        "ëª¨ë“  ì¶œë ¥ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì‹œì˜¤."
    )

    user_msg = f"""ì œëª©: {clean_text(title)}
ë‚´ìš©: {clean_text(content)}

### ì¶œë ¥ í˜•ì‹
[
  {{"query": "ì§ˆë¬¸", "answer": "ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€"}}
]

### ìƒì„± ê·œì¹™
1. ì§ˆë¬¸ì€ ë°˜ë“œì‹œ ì£¼ì–´ì§„ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ëŒì´ ì‹¤ì œë¡œ ê¶ê¸ˆí•´í•  ë§Œí•œ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
2. ë‹µë³€ì€ ë°˜ë“œì‹œ í•´ë‹¹ ë¬¸ì„œì—ì„œ ì§ì ‘ í™•ì¸ ê°€ëŠ¥í•œ ì •ë³´ë§Œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
3. ë¬¸ì„œì—ì„œ ë‹µì´ ëª…í™•íˆ ì£¼ì–´ì§€ì§€ ì•Šì€ ê²½ìš°, ë‹µë³€ì€ "í•´ë‹¹ ë¬¸ì„œì— ì—†ìŒ"ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
4. ì§ˆë¬¸ì€ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. ë‹¨ë¬¸ì´ë‚˜ ëª…ë ¹ë¬¸ í˜•íƒœëŠ” í”¼í•˜ì„¸ìš”.
5. ì§ˆë¬¸ì—ëŠ” 'ì´', 'ì´ë²ˆ', 'í•´ë‹¹', 'ì´ ì œí’ˆ'ê³¼ ê°™ì€ ëª¨í˜¸í•œ ì§€ì‹œì–´ ëŒ€ì‹ , ì§ˆë¬¸ì˜ ëŒ€ìƒì´ ëª…í™•íˆ ë“œëŸ¬ë‚˜ë„ë¡ í•˜ì„¸ìš”. ë‹¤ë§Œ, ì§ˆë¬¸ì´ ê³¼ë„í•˜ê²Œ ê¸¸ì–´ì§€ì§€ ì•Šë„ë¡ ì œí’ˆëª…ì´ë‚˜ ê¸°ê´€ëª… ë“±ì€ ê°„ê²°í•˜ê²Œ í•„ìš”í•œ ë§Œí¼ë§Œ í¬í•¨í•˜ì„¸ìš”.
6. ì§ˆë¬¸(query)ì— ë‹µë³€(answer)ì˜ ë‚´ìš©ì´ í¬í•¨ë˜ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤.
"""
    messages = [
        {"role": "system", "content": system_msg},
        {"role": "user", "content": user_msg}
    ]

    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)


# ğŸ” ëª¨ë¸ ì‘ë‹µì—ì„œ JSON ë°°ì—´ë§Œ ì¶”ì¶œ
def extract_json_from_response(response_text):
    try:
        match = re.search(r"\[\s*{.*?}\s*]", response_text, re.DOTALL)
        if match:
            json_str = match.group()
            return json.loads(json_str)
        else:
            raise ValueError("JSON í˜•ì‹ì˜ QA ìŒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print("JSON íŒŒì‹± ì‹¤íŒ¨:", e)
        return []

# ğŸ§  LLaMAë¥¼ ì´ìš©í•œ QA ì¶”ì¶œ
def get_qa_from_document(title, content):
    prompt = make_chat_prompt(title, content)

    try:
        result = llama_pipeline(prompt, max_new_tokens=512, do_sample=True, temperature=0.5)[0]["generated_text"]
        response_text = result.strip()
        print("=== ì›ë³¸ ì‘ë‹µ ===")
        print(response_text)

        qa_list = extract_json_from_response(response_text)
        print("=== ì¶”ì¶œëœ QA ===")
        print(qa_list)

        return qa_list

    except Exception as e:
        print("QA ìƒì„± ì‹¤íŒ¨:", e)
        print("ì›ë³¸ ì‘ë‹µ:", result if 'result' in locals() else "ì—†ìŒ")
        return []

# CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
file_path = "/home/food/people/minju/data/make_qa/real_final.csv"
df = pd.read_csv(file_path)

# QA ê²°ê³¼ ì»¬ëŸ¼ ì´ˆê¸°í™”
df["llama_query"] = ""
df["llama_answer"] = ""

# ë£¨í”„ ëŒë©° QA ìƒì„±
for i in range(len(df)):
    print(f"\n\\ [{i+1}/{len(df)}] QA ìƒì„± ì¤‘...")

    title = df.loc[i, "title"] if "title" in df.columns else ""
    if "document" in df.columns:
        text = df.loc[i, "document"]
    elif "extracted_text" in df.columns:
        text = df.loc[i, "extracted_text"]
    else:
        print("ë¬¸ì„œ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
        continue

    if pd.isna(text) or len(str(text).strip()) < 5:
        print("ë¬¸ì„œê°€ ë¹„ì–´ ìˆìŒ. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
        continue

    qa_pairs = get_qa_from_document(title, text)

    if qa_pairs:
        queries = [pair["query"] for pair in qa_pairs]
        answers = [pair["answer"] for pair in qa_pairs]
        df.at[i, "llama_query"] = "\n".join(queries)
        df.at[i, "llama_answer"] = "\n".join(answers)

    time.sleep(1)

# ê²°ê³¼ ì €ì¥
df.to_csv("real_final_with_llama_QA.csv", index=False)
print("\nì €ì¥ ì™„ë£Œ: real_final_with_llama_QA.csv")
